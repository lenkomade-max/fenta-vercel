# –¢–æ–ø-20 AI –º–æ–¥–µ–ª–µ–π OpenRouter (Q4 2025)

> –ê–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π 2025 –≥–æ–¥–∞ –∏ –∏—Ö —Ü–µ–Ω—ã –Ω–∞ 7 –¥–µ–∫–∞–±—Ä—è 2025 –≥.

**–ò—Å—Ç–æ—á–Ω–∏–∫:** [OpenRouter.ai](https://openrouter.ai/models)

---

## üìä –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞

| ‚Ññ | –ú–æ–¥–µ–ª—å | ID | Input (1M) | Output (1M) | –ö–æ–Ω—Ç–µ–∫—Å—Ç |
|---|--------|-----|------------|-------------|----------|
| 1 | OpenAI: GPT-5.1-Codex-Max | `openai/gpt-5.1-codex-max` | $1.25 | $10.00 | 400K |
| 2 | OpenAI: GPT-5.1 | `openai/gpt-5.1` | $1.25 | $10.00 | 400K |
| 3 | OpenAI: GPT-5.1 Chat | `openai/gpt-5.1-chat` | $1.25 | $10.00 | 128K |
| 4 | OpenAI: GPT-5.1-Codex | `openai/gpt-5.1-codex` | $1.25 | $10.00 | 400K |
| 5 | OpenAI: GPT-5.1-Codex-Mini | `openai/gpt-5.1-codex-mini` | $0.25 | $2.00 | 400K |
| 6 | OpenAI: GPT-5 Pro | `openai/gpt-5-pro` | $15.00 | $120.00 | 400K |
| 7 | OpenAI: GPT-5 Codex | `openai/gpt-5-codex` | $1.25 | $10.00 | 400K |
| 8 | OpenAI: GPT-5 Chat | `openai/gpt-5-chat` | $1.25 | $10.00 | 128K |
| 9 | OpenAI: GPT-5 | `openai/gpt-5` | $1.25 | $10.00 | 400K |
| 10 | OpenAI: GPT-5 Mini | `openai/gpt-5-mini` | $0.25 | $2.00 | 400K |
| 11 | OpenAI: GPT-5 Nano | `openai/gpt-5-nano` | $0.05 | $0.40 | 400K |
| 12 | Anthropic: Claude Opus 4.5 | `anthropic/claude-opus-4.5` | $5.00 | $25.00 | 200K |
| 13 | DeepSeek: DeepSeek V3.2 Speciale | `deepseek/deepseek-v3.2-speciale` | $0.27 | $0.41 | 164K |
| 14 | DeepSeek: DeepSeek V3.2 | `deepseek/deepseek-v3.2` | $0.27 | $0.40 | 164K |
| 15 | DeepSeek: DeepSeek V3.2 Exp | `deepseek/deepseek-v3.2-exp` | $0.21 | $0.32 | 164K |
| 16 | DeepSeek: DeepSeek V3.1 Terminus (exacto) | `deepseek/deepseek-v3.1-terminus:exacto` | $0.21 | $0.79 | 164K |
| 17 | DeepSeek: DeepSeek V3.1 Terminus | `deepseek/deepseek-v3.1-terminus` | $0.21 | $0.79 | 164K |
| 18 | Google: Gemini 3 Pro Preview | `google/gemini-3-pro-preview` | $2.00 | $12.00 | 1049K |
| 19 | Google: Gemini 2.5 Flash Preview 09-2025 | `google/gemini-2.5-flash-preview-09-2025` | $0.30 | $2.50 | 1049K |
| 20 | Google: Gemini 2.5 Flash Lite Preview 09-2025 | `google/gemini-2.5-flash-lite-preview-09-2025` | $0.10 | $0.40 | 1049K |

---

## üìù –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

### 1. OpenAI: GPT-5.1-Codex-Max

- **ID:** `openai/gpt-5.1-codex-max`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5.1-Codex-Max is OpenAI‚Äôs latest agentic coding model, designed for long-running, high-context software development tasks. It is based on an updated version of the 5.1 reasoning stack and trained on agentic workflows spanning software engineering, mathematics, and research.  GPT-5.1-Codex-Max delivers faster performance, improved reasoning, and higher token efficiency across the development lifecycle.
- **–¶–µ–Ω—ã:**
  - Input: $1.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $10.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 128,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 04.12.2025

### 2. OpenAI: GPT-5.1

- **ID:** `openai/gpt-5.1`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5.1 is the latest frontier-grade model in the GPT-5 series, offering stronger general-purpose reasoning, improved instruction adherence, and a more natural conversational style compared to GPT-5. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks. The model produces clearer, more grounded explanations with reduced jargon, making it easier to follow even on technical or multi-step problems. Built for broad task coverage, GPT-5.1 delivers consistent gains across math, coding, and structured analysis workloads, with more coherent long-form answers and improved tool-use reliability. It also features refined conversational alignment, enabling warmer, more intuitive responses without compromising precision. GPT-5.1 serves as the primary full-capability successor to GPT-5
- **–¶–µ–Ω—ã:**
  - Input: $1.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $10.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 128,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 13.11.2025

### 3. OpenAI: GPT-5.1 Chat

- **ID:** `openai/gpt-5.1-chat`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5.1 Chat (AKA Instant is the fast, lightweight member of the 5.1 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively ‚Äúthink‚Äù on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.1 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.
- **–¶–µ–Ω—ã:**
  - Input: $1.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $10.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 128,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 16,384
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 13.11.2025

### 4. OpenAI: GPT-5.1-Codex

- **ID:** `openai/gpt-5.1-codex`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5.1-Codex is a specialized version of GPT-5.1 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5.1, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level) Codex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically‚Äîproviding fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.
- **–¶–µ–Ω—ã:**
  - Input: $1.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $10.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 128,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 13.11.2025

### 5. OpenAI: GPT-5.1-Codex-Mini

- **ID:** `openai/gpt-5.1-codex-mini`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5.1-Codex-Mini is a smaller and faster version of GPT-5.1-Codex
- **–¶–µ–Ω—ã:**
  - Input: $0.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $2.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 100,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 13.11.2025

### 6. OpenAI: GPT-5 Pro

- **ID:** `openai/gpt-5-pro`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5 Pro is OpenAI‚Äôs most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like "think hard about this." Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.
- **–¶–µ–Ω—ã:**
  - Input: $15.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $120.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 128,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 06.10.2025

### 7. OpenAI: GPT-5 Codex

- **ID:** `openai/gpt-5-codex`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level) Codex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically‚Äîproviding fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.
- **–¶–µ–Ω—ã:**
  - Input: $1.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $10.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 128,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 23.09.2025

### 8. OpenAI: GPT-5 Chat

- **ID:** `openai/gpt-5-chat`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conversations for enterprise applications.
- **–¶–µ–Ω—ã:**
  - Input: $1.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $10.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 128,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 16,384
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 07.08.2025

### 9. OpenAI: GPT-5

- **ID:** `openai/gpt-5`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5 is OpenAI‚Äôs most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like "think hard about this." Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.
- **–¶–µ–Ω—ã:**
  - Input: $1.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $10.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 128,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 07.08.2025

### 10. OpenAI: GPT-5 Mini

- **ID:** `openai/gpt-5-mini`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost. GPT-5 Mini is the successor to OpenAI's o4-mini model.
- **–¶–µ–Ω—ã:**
  - Input: $0.25 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $2.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 128,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 07.08.2025

### 11. OpenAI: GPT-5 Nano

- **ID:** `openai/gpt-5-nano`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.
- **–¶–µ–Ω—ã:**
  - Input: $0.05 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $0.40 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 400,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 128,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 07.08.2025

### 12. Anthropic: Claude Opus 4.5

- **ID:** `anthropic/claude-opus-4.5`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** Claude Opus 4.5 is Anthropic‚Äôs frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high. Opus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.
- **–¶–µ–Ω—ã:**
  - Input: $5.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $25.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 200,000 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 32,000
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 24.11.2025

### 13. DeepSeek: DeepSeek V3.2 Speciale

- **ID:** `deepseek/deepseek-v3.2-speciale`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for maximum reasoning and agentic performance. It builds on DeepSeek Sparse Attention (DSA) for efficient long-context processing, then scales post-training reinforcement learning to push capability beyond the base model. Reported evaluations place Speciale ahead of GPT-5 on difficult reasoning workloads, with proficiency comparable to Gemini-3.0-Pro, while retaining strong coding and tool-use reliability. Like V3.2, it benefits from a large-scale agentic task synthesis pipeline that improves compliance and generalization in interactive environments.
- **–¶–µ–Ω—ã:**
  - Input: $0.27 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $0.41 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 163,840 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text->text
- **–ú–∞–∫—Å. completion tokens:** 65,536
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 01.12.2025

### 14. DeepSeek: DeepSeek V3.2

- **ID:** `deepseek/deepseek-v3.2`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with strong reasoning and agentic tool-use performance. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism that reduces training and inference cost while preserving quality in long-context scenarios. A scalable reinforcement learning post-training framework further improves reasoning, with reported performance in the GPT-5 class, and the model has demonstrated gold-medal results on the 2025 IMO and IOI. V3.2 also uses a large-scale agentic task synthesis pipeline to better integrate reasoning into tool-use settings, boosting compliance and generalization in interactive environments. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)
- **–¶–µ–Ω—ã:**
  - Input: $0.27 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $0.40 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 163,840 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text->text
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 01.12.2025

### 15. DeepSeek: DeepSeek V3.2 Exp

- **ID:** `deepseek/deepseek-v3.2-exp`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism designed to improve training and inference efficiency in long-context scenarios while maintaining output quality. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) The model was trained under conditions aligned with V3.1-Terminus to enable direct comparison. Benchmarking shows performance roughly on par with V3.1 across reasoning, coding, and agentic tool-use tasks, with minor tradeoffs and gains depending on the domain. This release focuses on validating architectural optimizations for extended context lengths rather than advancing raw task accuracy, making it primarily a research-oriented model for exploring efficient transformer designs.
- **–¶–µ–Ω—ã:**
  - Input: $0.21 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $0.32 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 163,840 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text->text
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 29.09.2025

### 16. DeepSeek: DeepSeek V3.1 Terminus (exacto)

- **ID:** `deepseek/deepseek-v3.1-terminus:exacto`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) The model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows.
- **–¶–µ–Ω—ã:**
  - Input: $0.21 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $0.79 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 163,840 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text->text
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 22.09.2025

### 17. DeepSeek: DeepSeek V3.1 Terminus

- **ID:** `deepseek/deepseek-v3.1-terminus`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config) The model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows.
- **–¶–µ–Ω—ã:**
  - Input: $0.21 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $0.79 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 163,840 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text->text
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 22.09.2025

### 18. Google: Gemini 3 Pro Preview

- **ID:** `google/gemini-3-pro-preview`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** Gemini 3 Pro is Google‚Äôs flagship frontier model for high-precision multimodal reasoning, combining strong performance across text, image, video, audio, and code with a 1M-token context window. Reasoning Details must be preserved when using multi-turn tool calling, see our docs here: https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks. It delivers state-of-the-art benchmark results in general reasoning, STEM problem solving, factual QA, and multimodal understanding, including leading scores on LMArena, GPQA Diamond, MathArena Apex, MMMU-Pro, and Video-MMMU. Interactions emphasize depth and interpretability: the model is designed to infer intent with minimal prompting and produce direct, insight-focused responses. Built for advanced development and agentic workflows, Gemini 3 Pro provides robust tool-calling, long-horizon planning stability, and strong zero-shot generation for complex UI, visualization, and coding tasks. It excels at agentic coding (SWE-Bench Verified, Terminal-Bench 2.0), multimodal analysis, and structured long-form tasks such as research synthesis, planning, and interactive learning experiences. Suitable applications include autonomous agents, coding assistants, multimodal analytics, scientific reasoning, and high-context information processing.
- **–¶–µ–Ω—ã:**
  - Input: $2.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $12.00 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 1,048,576 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 65,536
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 18.11.2025

### 19. Google: Gemini 2.5 Flash Preview 09-2025

- **ID:** `google/gemini-2.5-flash-preview-09-2025`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in "thinking" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling.  Additionally, Gemini 2.5 Flash is configurable through the "max tokens for reasoning" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).
- **–¶–µ–Ω—ã:**
  - Input: $0.30 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $2.50 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 1,048,576 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 65,536
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 25.09.2025

### 20. Google: Gemini 2.5 Flash Lite Preview 09-2025

- **ID:** `google/gemini-2.5-flash-lite-preview-09-2025`
- **–û–ø–∏—Å–∞–Ω–∏–µ:** Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, "thinking" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence.
- **–¶–µ–Ω—ã:**
  - Input: $0.10 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
  - Output: $0.40 –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ:** 1,048,576 —Ç–æ–∫–µ–Ω–æ–≤
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** text+image->text
- **–ú–∞–∫—Å. completion tokens:** 65,536
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 25.09.2025

---

## üí° –ü—Ä–∏–º–µ—á–∞–Ω–∏—è

- –¶–µ–Ω—ã —É–∫–∞–∑–∞–Ω—ã –≤ USD –∑–∞ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
- –ú–æ–¥–µ–ª–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ 2025 –≥–æ–¥–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã)
- –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: 07.12.2025
- –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∞–º–æ–π –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ—Å–µ—Ç–∏—Ç–µ [OpenRouter.ai](https://openrouter.ai/models)
